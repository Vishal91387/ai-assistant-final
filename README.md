# ğŸ§  AI Suite Home

A personal, local AI assistant built using **Llama 3**, **LangChain**, and **Streamlit**, with real-time access to:
- ğŸ“„ Your own documents (RAG-based retrieval)
- ğŸŒ Live web search (via external APIs)
- ğŸ—‚ï¸ File upload, document parsing, and chunk preview
- ğŸ“¤ Chat log export to PDF or Word

---

## ğŸš€ Features

- ğŸ” **RAG (Retrieval-Augmented Generation):**
  - Ask questions based on PDFs, Word, and text files from your local `docs/` folder
  - Extract answers with source tracking and chunk previews

- ğŸŒ **Web Agent Integration:**
  - Fetch and summarize content from sources like Wikipedia, Serper, or Arxiv

- ğŸ§¾ **Chat Export:**
  - Download your Q&A session as a formatted PDF or Word file

- ğŸ§  **Powered by Llama 3:**
  - Local inference via `Ollama`, using `llama3:8b` for privacy and speed

---

## ğŸ“ Project Structure

```
my_ai_suite/
â”œâ”€â”€ streamlit_app.py         # Main Streamlit UI
â”œâ”€â”€ ingest_docs.py           # Document loader and chunker
â”œâ”€â”€ rag_chat.py              # Handles RAG-style QA
â”œâ”€â”€ agent_chat.py            # Web-based agent
â”œâ”€â”€ tools/                   # Custom tools (Wikipedia, Serper, Arxiv)
â”œâ”€â”€ utils.py                 # PDF/Word export, summarizers
â”œâ”€â”€ docs/                    # Upload your reference files here
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ start.sh                 # Optional startup script
â””â”€â”€ .gitignore               # Tracks only what matters
```

---

## ğŸ› ï¸ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/Vishal91387/ai-suite-home.git
cd ai-suite-home
```

### 2. Create a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Start the Streamlit app

```bash
streamlit run streamlit_app.py
```

---

## ğŸ“‚ Upload Your Files

Drag and drop `.pdf`, `.docx`, or `.txt` files into the Streamlit sidebar, or place them directly into the `/docs` folder. The app will auto-ingest and chunk the content for RAG search.

---

## ğŸ“„ Export Options

- Click `ğŸ“„ PDF` or `ğŸ“ Word` to export your full chat history
- TL;DR summaries and bullet points are included in export

---

## ğŸ§  LLM Setup (Ollama)

Make sure Ollama is installed and running locally:

```bash
ollama run llama3:8b
```

You can change models by editing the embedding and agent files (`rag_chat.py`, `agent_chat.py`).

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE) â€“ feel free to fork, modify, and build on it!

---

## ğŸ™Œ Credits

Built by [Vishal Sharma](https://github.com/Vishal91387)  
Powered by **LangChain**, **Streamlit**, and **Llama 3**
